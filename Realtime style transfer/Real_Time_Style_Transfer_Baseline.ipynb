{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "import torchvision as tv\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torch.onnx\n",
    "from matplotlib import pyplot as plt\n",
    "import utils\n",
    "from vgg import Vgg16\n",
    "import nntools as nt\n",
    "from torch import nn\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformation_net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(transformation_net, self).__init__()\n",
    "        \n",
    "        # Initial Downsampling layers\n",
    "\n",
    "        self.conv1 = Conv_Layer(3, 32, 9, stride=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(32, affine=True)\n",
    "\n",
    "        self.conv2 = Conv_Layer(32, 64, 3, stride=2)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(64, affine=True)\n",
    "\n",
    "        self.conv3 = Conv_Layer(64, 128, 3, stride=2)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(128, affine=True)\n",
    "\n",
    "        # Residual layers\n",
    "        self.res1 = Res_block(128)\n",
    "        self.res2 = Res_block(128)\n",
    "        self.res3 = Res_block(128)\n",
    "        self.res4 = Res_block(128)\n",
    "        self.res5 = Res_block(128)\n",
    "        \n",
    "        # Upsampling Layers\n",
    "        self.deconv1 = Conv_Layer(128, 64, kernel_size=3, stride=1, upsample=2)\n",
    "        self.bn4 = torch.nn.BatchNorm2d(64, affine=True)\n",
    "\n",
    "        self.deconv2 = Conv_Layer(64, 32, kernel_size=3, stride=1, upsample=2)\n",
    "        self.bn5 = torch.nn.BatchNorm2d(32, affine=True)\n",
    "\n",
    "        self.deconv3 = Conv_Layer(32, 3, kernel_size=9, stride=1)\n",
    "    \n",
    "        # Residual Layers\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.relu(self.bn1(self.conv1(X)))\n",
    "        y = self.relu(self.bn2(self.conv2(y)))\n",
    "        y = self.relu(self.bn3(self.conv3(y)))\n",
    "        y = self.res1(y)\n",
    "        y = self.res2(y)\n",
    "        y = self.res3(y)\n",
    "        y = self.res4(y)\n",
    "        y = self.res5(y)\n",
    "        y = self.relu(self.bn4(self.deconv1(y)))\n",
    "        y = self.relu(self.bn5(self.deconv2(y)))\n",
    "        y = self.deconv3(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class Conv_Layer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
    "        super(Conv_Layer, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_in = x\n",
    "        if self.upsample:\n",
    "            x_in = torch.nn.functional.interpolate(x_in, mode='nearest', scale_factor=self.upsample)\n",
    "        h = self.reflection_pad(x_in)\n",
    "        h = self.conv(h)\n",
    "        return h\n",
    "    \n",
    "class Res_block(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, channels):\n",
    "        super(Res_block, self).__init__()\n",
    "        self.conv1 = Conv_Layer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(channels, affine=True)\n",
    "        self.conv2 = Conv_Layer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(channels, affine=True)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.bn1(self.conv1(x)))\n",
    "        h = self.bn2(self.conv2(h))\n",
    "        h = h + x\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(args.image_size),\n",
    "        transforms.CenterCrop(args.image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.mul(255))\n",
    "    ])\n",
    "    \n",
    "    train_dataset = datasets.ImageFolder(args.dataset, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size)\n",
    "\n",
    "    transformer = transformation_net().to(device)\n",
    "    optimizer = Adam(transformer.parameters(), args.lr)\n",
    "    mse = torch.nn.MSELoss()\n",
    "    vgg = Vgg16(requires_grad=False).to(device)\n",
    "    \n",
    "    style_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.mul(255))\n",
    "    ])\n",
    "    \n",
    "    style_img = utils.imread(args.style_image, size=args.style_size)\n",
    "    style_img = style_transform(style_img)\n",
    "    style_img = style_img.repeat(args.batch_size, 1, 1, 1).to(device)\n",
    "\n",
    "    layer_features_style = vgg(utils.normalize_imageset(style_img))\n",
    "    gram_matrix_style = [utils.gram_matrix(y) for y in layer_features_style]\n",
    "    \n",
    "    plot_index = 0 \n",
    "    x_index_vals = []\n",
    "    y_index_vals = []\n",
    "\n",
    "    for e in range(args.epoch_count):\n",
    "        transformer.train()\n",
    "        total_content_loss = 0.\n",
    "        total_style_loss = 0.\n",
    "        count = 0\n",
    "        \n",
    "        for batch_index, (x, _) in enumerate(train_loader):\n",
    "            \n",
    "            plot_index += 1\n",
    "            n_batch = len(x)\n",
    "            count += n_batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = transformer(x)\n",
    "\n",
    "            y = utils.normalize_imageset(y)\n",
    "            x = utils.normalize_imageset(x)\n",
    "\n",
    "            features_y = vgg(y)\n",
    "            features_x = vgg(x)\n",
    "\n",
    "            content_loss = args.content_weight * mse(features_y['layer4'], features_x['layer4'])\n",
    "\n",
    "            style_loss = 0.\n",
    "            for ft_y, gm_s in zip(features_y, gram_matrix_style):\n",
    "                gm_y = [utils.gram_matrix(layer_features_style[y]) for y in ft_y]\n",
    "                style_loss += mse(gm_y, gm_s[:n_batch, :, :])\n",
    "            style_loss *= args.style_weight\n",
    "\n",
    "            total_loss = content_loss + style_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_content_loss += content_loss.item()\n",
    "            total_style_loss += style_loss.item()\n",
    "            \n",
    "            total_loss = (total_content_loss + total_style_loss) / (batch_index + 1)\n",
    "\n",
    "            if (batch_index + 1) % args.log_interval == 0:\n",
    "                mesg = \"{}\\tEpoch {}:\\t[{}/{}]\\tcontent: {:.6f}\\tstyle: {:.6f}\\ttotal: {:.6f}\".format(\n",
    "                    time.ctime(), e + 1, count, len(train_dataset),\n",
    "                                  total_content_loss / (batch_index + 1),\n",
    "                                  total_style_loss / (batch_index + 1), total_loss)\n",
    "                \n",
    "                print(mesg)\n",
    "                \n",
    "            y_index_vals.append(total_loss)\n",
    "            x_index_vals.append(plot_index)\n",
    "\n",
    "            if args.checkpoint_model_dir is not None and (batch_index + 1) % args.checkpoint_interval == 0:\n",
    "                transformer.eval().cpu()\n",
    "                ckpt_model_filename = \"ckpt_epoch_\" + str(e) + \"_batch_index_\" + str(batch_index + 1) + \".pth\"\n",
    "                ckpt_model_path = os.path.join(args.checkpoint_model_dir, ckpt_model_filename)\n",
    "                torch.save(transformer.state_dict(), ckpt_model_path)\n",
    "                transformer.to(device).train()\n",
    "            \n",
    "\n",
    "    # save model\n",
    "    transformer.eval().cpu()\n",
    "    save_model_filename = \"epoch_\" + str(args.epoch_count) + \"_\" + str(time.ctime()).replace(' ', '_') + \"_\" + str(\n",
    "        args.content_weight) + \"_\" + str(args.style_weight) + \".pth\"\n",
    "    save_model_path = os.path.join(args.save_model_dir, save_model_filename)\n",
    "    torch.save(transformer.state_dict(), save_model_path)\n",
    "\n",
    "    print(\"\\nDone, trained model saved at\", save_model_path)\n",
    "    print('\\ngoing to pickle nd plot')\n",
    "    \n",
    "    with open('x_index.pkl', 'wb') as f:\n",
    "        pickle.dump(x_index_vals, f)\n",
    "    with open('y_index.pkl', 'wb') as f:\n",
    "        pickle.dump(y_index_vals, f)\n",
    "\n",
    "def stylize(args):\n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "    content_image = utils.imread(args.content_image, scale=args.content_scale)\n",
    "    content_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.mul(255))\n",
    "    ])\n",
    "    content_image = content_transform(content_image)\n",
    "    content_image = content_image.unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "    print (\"Realised it's a pytorch model\")\n",
    "    with torch.no_grad():\n",
    "        style_model = transformation_net()\n",
    "        style_model.eval()\n",
    "        state_dict = torch.load(args.model)\n",
    "\n",
    "        print (\"Found the model!\")\n",
    "#             remove saved deprecated running_* keys in InstanceNorm from the checkpoint\n",
    "        for k in list(state_dict.keys()):\n",
    "           if re.search(r'in\\d+\\.running_(mean|var)$', k):\n",
    "                del state_dict[k]\n",
    "        style_model.load_state_dict(state_dict)\n",
    "        style_model.to(device)\n",
    "        print (\"Loaded the model\")\n",
    "\n",
    "        output = style_model(content_image).cpu()\n",
    "    utils.save_image(args.output_image, output[0])\n",
    "\n",
    "    print (\"SAved image\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_train:\n",
    "    epoch_count = 2\n",
    "    batch_size = 4\n",
    "    dataset = '/datasets/home/65/465/ssreekri/examples/fast_neural_style/train2014_new/'\n",
    "    style_image = '/datasets/home/65/465/ssreekri/examples/fast_neural_style/images/style-images/starry_night.jpg'\n",
    "    save_model_dir = '/datasets/home/65/465/ssreekri/examples/fast_neural_style/saved_models'\n",
    "    checkpoint_model_dir = '../saved_models'\n",
    "    image_size = 256\n",
    "    style_size = None\n",
    "    cuda = 1\n",
    "    seed = 42\n",
    "    content_weight = 1e5\n",
    "    style_weight = 1e10\n",
    "    lr = 1e-3\n",
    "    log_interval = 500\n",
    "    checkpoint_interval = 2000\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e988c8249243>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArgs_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-bd76ae1a9262>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mfeatures_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mcontent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layer4'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layer4'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mstyle_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "args = Args_train()\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalz=Args_eval()\n",
    "# stylize(evalz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('x_index.pkl', 'rb') as f:\n",
    "...   xxx = pickle.load(f)\n",
    "\n",
    "with open('y_index.pkl', 'rb') as f:\n",
    "...   yyy = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(yyy,xxx)\n",
    "plt.xlabel('Training Samples ----->')\n",
    "plt.ylabel('Total Loss----->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Args_eval:\n",
    "    content_image = '/datasets/home/65/465/ssreekri/examples/fast_neural_style/images/style-images/rain-princess.jpg'\n",
    "    content_scale = None\n",
    "    output_image = '/datasets/home/65/465/ssreekri/examples/fast_neural_style/images/output-images/output_autumn_starry_in.jpg'\n",
    "    model = '/datasets/home/65/465/ssreekri/examples/fast_neural_style/saved_models/Starry_night_Instance_Norm.pth'\n",
    "    cuda = 1\n",
    "\n",
    "args=Args_eval()\n",
    "stylize(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
