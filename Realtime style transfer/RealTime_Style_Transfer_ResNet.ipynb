{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real- Time Style Transfer Using ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "import torchvision as tv\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torch.onnx\n",
    "from matplotlib import pyplot as plt\n",
    "import utils\n",
    "from vgg import Vgg16\n",
    "import nntools as nt\n",
    "from torch import nn\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation Net Class Definition - Baseline Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformation_net_batch(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(transformation_net_batch, self).__init__()\n",
    "        \n",
    "        # Initial Downsampling layers\n",
    "\n",
    "        self.conv1 = Conv_Layer(3, 32, 9, stride=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(32, affine=True)\n",
    "\n",
    "        self.conv2 = Conv_Layer(32, 64, 3, stride=2)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(64, affine=True)\n",
    "\n",
    "        self.conv3 = Conv_Layer(64, 128, 3, stride=2)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(128, affine=True)\n",
    "\n",
    "        # Residual layers\n",
    "        self.res1 = Res_block_Batch(128)\n",
    "        self.res2 = Res_block_Batch(128)\n",
    "        self.res3 = Res_block_Batch(128)\n",
    "        self.res4 = Res_block_Batch(128)\n",
    "        self.res5 = Res_block_Batch(128)\n",
    "        \n",
    "        # Final Upsampling Layers\n",
    "        self.deconv1 = Conv_Layer(128, 64, kernel_size=3, stride=1, upsample=2)\n",
    "        self.bn4 = torch.nn.BatchNorm2d(64, affine=True)\n",
    "\n",
    "        self.deconv2 = Conv_Layer(64, 32, kernel_size=3, stride=1, upsample=2)\n",
    "        self.bn5 = torch.nn.BatchNorm2d(32, affine=True)\n",
    "\n",
    "        self.deconv3 = Conv_Layer(32, 3, kernel_size=9, stride=1)\n",
    "    \n",
    "        # Residual Layers\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.relu(self.bn1(self.conv1(X)))\n",
    "        y = self.relu(self.bn2(self.conv2(y)))\n",
    "        y = self.relu(self.bn3(self.conv3(y)))\n",
    "        y = self.res1(y)\n",
    "        y = self.res2(y)\n",
    "        y = self.res3(y)\n",
    "        y = self.res4(y)\n",
    "        y = self.res5(y)\n",
    "        y = self.relu(self.bn4(self.deconv1(y)))\n",
    "        y = self.relu(self.bn5(self.deconv2(y)))\n",
    "        y = self.deconv3(y)\n",
    "        return y\n",
    "    \n",
    "class Res_block_Batch(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, channels):\n",
    "        super(Res_block_Batch, self).__init__()\n",
    "        self.conv1 = Conv_Layer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(channels, affine=True)\n",
    "        self.conv2 = Conv_Layer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(channels, affine=True)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.bn1(self.conv1(x)))\n",
    "        h = self.bn2(self.conv2(h))\n",
    "        h = h + x\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation Net Class Definition - Replacing BatchNorm with Instance Normalization Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformation_net_instance(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(transformation_net_instance, self).__init__()\n",
    "        \n",
    "        # Initial Downsampling layers\n",
    "\n",
    "        self.conv1 = Conv_Layer(3, 32, 9, stride=1)\n",
    "        self.in1 = torch.nn.InstanceNorm2d(32, affine=True)\n",
    "\n",
    "        self.conv2 = Conv_Layer(32, 64, 3, stride=2)\n",
    "        self.in2 = torch.nn.InstanceNorm2d(64, affine=True)\n",
    "\n",
    "        self.conv3 = Conv_Layer(64, 128, 3, stride=2)\n",
    "        self.in3 = torch.nn.InstanceNorm2d(128, affine=True)\n",
    "\n",
    "        # Residual layers\n",
    "        self.res1 = Res_block_Instance(128)\n",
    "        self.res2 = Res_block_Instance(128)\n",
    "        self.res3 = Res_block_Instance(128)\n",
    "        self.res4 = Res_block_Instance(128)\n",
    "        self.res5 = Res_block_Instance(128)\n",
    "        \n",
    "        # Final Upsampling Layers\n",
    "        self.deconv1 = Conv_Layer(128, 64, kernel_size=3, stride=1, upsample=2)\n",
    "        self.in4 = torch.nn.InstanceNorm2d(64, affine=True)\n",
    "\n",
    "        self.deconv2 = Conv_Layer(64, 32, kernel_size=3, stride=1, upsample=2)\n",
    "        self.in5 = torch.nn.InstanceNorm2d(32, affine=True)\n",
    "\n",
    "        self.deconv3 = Conv_Layer(32, 3, kernel_size=9, stride=1)\n",
    "    \n",
    "        # Residual Layers\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.relu(self.in1(self.conv1(X)))\n",
    "        y = self.relu(self.in2(self.conv2(y)))\n",
    "        y = self.relu(self.in3(self.conv3(y)))\n",
    "        y = self.res1(y)\n",
    "        y = self.res2(y)\n",
    "        y = self.res3(y)\n",
    "        y = self.res4(y)\n",
    "        y = self.res5(y)\n",
    "        y = self.relu(self.in4(self.deconv1(y)))\n",
    "        y = self.relu(self.in5(self.deconv2(y)))\n",
    "        y = self.deconv3(y)\n",
    "        return y\n",
    "\n",
    "class Res_block_Instance(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, channels):\n",
    "        super(Res_block_Instance, self).__init__()\n",
    "        self.conv1 = Conv_Layer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.conv2 = Conv_Layer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in2 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.in1(self.conv1(x)))\n",
    "        h = self.in2(self.conv2(h))\n",
    "        h = h + x\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Layer for Transformation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_Layer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
    "        super(Conv_Layer, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_in = x\n",
    "        if self.upsample:\n",
    "            x_in = torch.nn.functional.interpolate(x_in, mode='nearest', scale_factor=self.upsample)\n",
    "        h = self.reflection_pad(x_in)\n",
    "        h = self.conv(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Transformation Net based on the Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(args.image_size),\n",
    "        transforms.CenterCrop(args.image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.mul(255))\n",
    "    ])\n",
    "    train_dataset = datasets.ImageFolder(args.dataset_path, transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size)\n",
    "\n",
    "    if args.Normalization == 'instance':\n",
    "        transformer = transformation_net_instance().to(device)\n",
    "    else:\n",
    "        transformer = transformation_net_batch().to(device)\n",
    "    \n",
    "    optimizer = Adam(transformer.parameters(), args.lr)\n",
    "    mse = torch.nn.MSELoss()\n",
    "\n",
    "    resnet = tv.models.resnet18(pretrained = True).to(device)\n",
    "    \n",
    "    for param in resnet.parameters():\n",
    "        param.requires_grad_(False)\n",
    "        \n",
    "    style_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.mul(255))\n",
    "    ])\n",
    "    \n",
    "    style_img = utils.imread(args.style_image, size=args.style_size)\n",
    "    style_img = style_transform(style_img)\n",
    "    style_img = style_img.repeat(args.batch_size, 1, 1, 1).to(device)\n",
    "\n",
    "    features_style = utils.get_resnet_features(resnet, style_img)\n",
    "    gram_style = [utils.gram_matrix(features_style[y]) for y in features_style]\n",
    "    \n",
    "    plot_index = 0 \n",
    "    x_index = []\n",
    "    y_index = []\n",
    "\n",
    "    for e in range(args.epochs):\n",
    "        transformer.train()\n",
    "        total_content_loss = 0.\n",
    "        total_style_loss = 0.\n",
    "        count = 0\n",
    "        \n",
    "        for batch_index, (content_batch, _) in enumerate(train_loader):\n",
    "            \n",
    "            plot_index += 1\n",
    "            n_batch = len(content_batch)\n",
    "            count += n_batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            content_batch = content_batch.to(device)\n",
    "            \n",
    "            y = transformer(content_batch)\n",
    "            \n",
    "            y = utils.normalize_imageset(y)\n",
    "            content_batch = utils.normalize_imageset(content_batch)\n",
    "\n",
    "            features_y = utils.get_resnet_features(resnet, content_batch)\n",
    "            features_x = utils.get_resnet_features(resnet, content_batch)\n",
    "        \n",
    "            content_loss = args.content_weight * mse(features_y['layer4'], features_x['layer4'])\n",
    "\n",
    "            style_loss = 0.\n",
    "            \n",
    "            for f_y, g_s in zip(features_y, gram_style):\n",
    "                g_y = utils.gram_matrix(features_y[f_y])\n",
    "                style_loss += mse(g_y, g_s[:n_batch, :, :])\n",
    "            style_loss *= args.style_weight\n",
    "\n",
    "            total_loss = content_loss + style_loss\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_content_loss += content_loss.item()\n",
    "            total_style_loss += style_loss.item()\n",
    "            \n",
    "            total_loss = (total_content_loss + total_style_loss) / (batch_index + 1)\n",
    "\n",
    "            if (batch_index + 1) % args.log_interval == 0:\n",
    "                message = \"{}\\tEpoch {}:\\t[{}/{}]\\tcontent: {:.6f}\\tstyle: {:.6f}\\ttotal: {:.6f}\".format(\n",
    "                    time.ctime(), e + 1, count, len(train_dataset),\n",
    "                                  total_content_loss / (batch_index + 1),\n",
    "                                  total_style_loss / (batch_index + 1), total_loss\n",
    "                )\n",
    "                print(message)\n",
    "                \n",
    "            y_index.append(total_loss)\n",
    "            x_index.append(plot_index)\n",
    "\n",
    "            if args.checkpoint_dir is not None and (batch_index + 1) % args.checkpoint_interval == 0:\n",
    "                transformer.eval().cpu()\n",
    "                ckpt_model_filename = \"ckpt_epoch_\" + str(e) + \"_batch_index_\" + str(batch_index + 1) + \".pth\"\n",
    "                ckpt_model_path = os.path.join(args.checkpoint_dir, ckpt_model_filename)\n",
    "                torch.save(transformer.state_dict(), ckpt_model_path)\n",
    "                transformer.to(device).train()\n",
    "            \n",
    "\n",
    "    # save model\n",
    "    \n",
    "    transformer.eval().cpu()\n",
    "    save_model_filename = \"model_1.pth\"\n",
    "    save_model_path = os.path.join(args.save_model_dir, save_model_filename)\n",
    "    torch.save(transformer.state_dict(), save_model_path)\n",
    "\n",
    "    print(\"\\nDone, trained model saved at\", save_model_path)\n",
    "    \n",
    "    with open('x_index_model1.pkl', 'wb') as f:\n",
    "        pickle.dump(x_index, f)\n",
    "    with open('y_index_model1.pkl', 'wb') as f:\n",
    "        pickle.dump(y_index, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stylize(args):\n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "    content_image = utils.imread(args.content_image, scale=args.content_scale)\n",
    "    content_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.mul(255))\n",
    "    ])\n",
    "    content_image = content_transform(content_image)\n",
    "    content_image = content_image.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if args.Normalization == 'instance':\n",
    "            style_model = transformation_net_instance()\n",
    "        else:\n",
    "            style_model = transformation_net_batch()\n",
    "        style_model.eval()\n",
    "        state_dict = torch.load(args.model)\n",
    "\n",
    "        print (\"Found the model!\")\n",
    "        if args.Normalization == 'instance':\n",
    "            for k in list(state_dict.keys()):\n",
    "                if re.search(r'in\\d+\\.running_(mean|var)$', k):\n",
    "                    del state_dict[k]\n",
    "                if re.search(r'conv\\d+\\.conv2d.(weight|bias)$', k):\n",
    "                    k1 = k.replace(\"2d\", \"\")\n",
    "                    state_dict[k1] = state_dict[k]\n",
    "                    del state_dict[k]\n",
    "\n",
    "        style_model.load_state_dict(state_dict)\n",
    "        style_model.to(device)\n",
    "        print (\"Loaded the model\")\n",
    "\n",
    "        output = style_model(content_image).cpu()\n",
    "    utils.save_image(args.output_image, output[0])\n",
    "\n",
    "    print (\"Saved image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_train:\n",
    "    epochs = 2\n",
    "    batch_size = 4\n",
    "    dataset_path = '/datasets/home/65/465/ssreekri/examples/fast_neural_style/train2014_new/'\n",
    "    style_image = '/datasets/home/65/465/ssreekri/examples/fast_neural_style/images/style-images/starry_night.jpg'\n",
    "    save_model_dir = '/datasets/home/65/465/ssreekri/examples/fast_neural_style/saved_models'\n",
    "    checkpoint_dir = '../saved_models'\n",
    "    image_size = 256\n",
    "    style_size = None\n",
    "    cuda = 1\n",
    "    content_weight = 1e5\n",
    "    style_weight = 1e10\n",
    "    lr = 1e-3\n",
    "    log_interval = 500\n",
    "    checkpoint_interval = 2000\n",
    "    Normalization = 'instance'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model with the arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args_train()\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the Total Loss Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('x_index_model1.pkl', 'rb') as f:\n",
    "...   x_axis = pickle.load(f)\n",
    "\n",
    "with open('y_index_model1.pkl', 'rb') as f:\n",
    "...   y_axis = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xaxis,yaxis)\n",
    "plt.xlabel('Training Samples ----->')\n",
    "plt.ylabel('Total Loss----->')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Args_eval:\n",
    "    content_image = '/datasets/home/65/465/ssreekri/examples/fast_neural_style/images/style-images/rain-princess.jpg'\n",
    "    content_scale = None\n",
    "    output_image = '/datasets/home/65/465/ssreekri/examples/fast_neural_style/images/output-images/output_autumn_starry_in.jpg'\n",
    "    model = '/datasets/home/65/465/ssreekri/examples/fast_neural_style/saved_models/candy.pth'\n",
    "    cuda = 1\n",
    "    Normalization = 'instance'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=Args_eval()\n",
    "test_stylize(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
